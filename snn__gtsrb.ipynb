{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a_kW-ctCuZbA",
        "outputId": "562a54a5-3c54-4f68-d67e-f41c06c75853"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n",
            "Tesla T4\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "print(torch.cuda.is_available())  # Should print 'True' if GPU is available\n",
        "print(torch.cuda.get_device_name(0))  # Prints the name of the GPU"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import snntorch as snn\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "dtype = torch.float\n",
        "\n",
        "# Define transformations for GTSRB dataset\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((32, 32)),  # Resize to match GTSRB dimensions\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1), # Data augmentation\n",
        "    transforms.RandomRotation(10),  # Data augmentation\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))  # Adjust normalization for GTSRB if necessary\n",
        "])\n",
        "\n",
        "# Load GTSRB dataset from torchvision\n",
        "data_path = './data/gtsrb'  # Set your data path\n",
        "train_data = datasets.GTSRB(root=data_path, split='train', download=False, transform=transform)\n",
        "test_data = datasets.GTSRB(root=data_path, split='test', download=False, transform=transform)\n",
        "train_loader = DataLoader(train_data, batch_size=64, shuffle=True, drop_last=True)  # Smaller batch size\n",
        "test_loader = DataLoader(test_data, batch_size=64, shuffle=True, drop_last=True)\n",
        "\n",
        "# Update model parameters for GTSRB\n",
        "num_inputs = 32 * 32 * 3  # Account for RGB channels\n",
        "num_hidden1 = 1200\n",
        "num_hidden2 = 800\n",
        "num_outputs = 43      # Number of classes in GTSRB\n",
        "\n",
        "# Temporal Dynamics\n",
        "num_steps = 50  # Increased for more temporal processing\n",
        "beta = 0.9\n",
        "\n",
        "# Define Network\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # Initialize layers\n",
        "        self.fc1 = nn.Linear(num_inputs, num_hidden1)\n",
        "        self.lif1 = snn.Leaky(beta=beta)\n",
        "        self.fc2 = nn.Linear(num_hidden1, num_hidden2)\n",
        "        self.lif2 = snn.Leaky(beta=beta)\n",
        "        self.fc3 = nn.Linear(num_hidden2, num_outputs)\n",
        "        self.lif3 = snn.Leaky(beta=beta)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Initialize hidden states at t=0\n",
        "        mem1 = self.lif1.init_leaky()\n",
        "        mem2 = self.lif2.init_leaky()\n",
        "        mem3 = self.lif3.init_leaky()\n",
        "\n",
        "        # Record the final layer\n",
        "        spk3_rec = []\n",
        "        mem3_rec = []\n",
        "\n",
        "        # Time-loop\n",
        "        for step in range(num_steps):\n",
        "            cur1 = self.fc1(x.flatten(1))  # Flatten to batch x (32*32*3)\n",
        "            spk1, mem1 = self.lif1(cur1, mem1)\n",
        "            cur2 = self.fc2(spk1)\n",
        "            spk2, mem2 = self.lif2(cur2, mem2)\n",
        "            cur3 = self.fc3(spk2)\n",
        "            spk3, mem3 = self.lif3(cur3, mem3)\n",
        "\n",
        "            # Store results\n",
        "            spk3_rec.append(spk3)\n",
        "            mem3_rec.append(mem3)\n",
        "\n",
        "        return torch.stack(spk3_rec, dim=0), torch.stack(mem3_rec, dim=0)  # time-steps x batch x num_outputs\n",
        "\n",
        "# Initialize network, loss function, and optimizer\n",
        "net = Net().to(device)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(net.parameters(), lr=1e-4)  # Decreased learning rate\n",
        "\n",
        "# Training parameters\n",
        "num_epochs = 10  # Increased epochs\n",
        "counter = 0\n",
        "\n",
        "# Outer training loop\n",
        "for epoch in range(num_epochs):\n",
        "    net.train()\n",
        "    for data, targets in train_loader:\n",
        "        data, targets = data.to(device), targets.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        spk_rec, _ = net(data)\n",
        "\n",
        "        # Compute loss over time\n",
        "        loss_val = loss_fn(spk_rec.sum(0), targets)  # Sum spikes over time, then apply CrossEntropy\n",
        "\n",
        "        # Backpropagation and optimization\n",
        "        optimizer.zero_grad()\n",
        "        loss_val.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Print loss\n",
        "        if counter % 100 == 0:\n",
        "            print(f\"Epoch: {epoch} \\t Iteration: {counter} \\t Train Loss: {loss_val.item()}\")\n",
        "        counter += 1\n",
        "\n",
        "# Testing the model\n",
        "net.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for data, targets in test_loader:\n",
        "        data, targets = data.to(device), targets.to(device)\n",
        "\n",
        "        # Forward pass through the network\n",
        "        spk_rec, _ = net(data)\n",
        "\n",
        "        # Sum the spikes over time and compute predictions\n",
        "        output_sum = spk_rec.sum(0)  # Sum over time\n",
        "        _, predicted = torch.max(output_sum, 1)\n",
        "\n",
        "        # Update correct predictions and total samples\n",
        "        correct += (predicted == targets).sum().item()\n",
        "        total += targets.size(0)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = 100 * correct / total\n",
        "print(f\"Test Accuracy: {accuracy:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nTDrK714udvP",
        "outputId": "d3958e6b-b575-4554-c847-1f822043f0d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://sid.erda.dk/public/archives/daaeac0d7ce1152aea9b61d9f1e19370/GTSRB-Training_fixed.zip to data/gtsrb/gtsrb/GTSRB-Training_fixed.zip\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 187M/187M [00:08<00:00, 22.3MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/gtsrb/gtsrb/GTSRB-Training_fixed.zip to data/gtsrb/gtsrb\n",
            "Downloading https://sid.erda.dk/public/archives/daaeac0d7ce1152aea9b61d9f1e19370/GTSRB_Final_Test_Images.zip to data/gtsrb/gtsrb/GTSRB_Final_Test_Images.zip\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 89.0M/89.0M [00:04<00:00, 19.6MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/gtsrb/gtsrb/GTSRB_Final_Test_Images.zip to data/gtsrb/gtsrb\n",
            "Downloading https://sid.erda.dk/public/archives/daaeac0d7ce1152aea9b61d9f1e19370/GTSRB_Final_Test_GT.zip to data/gtsrb/gtsrb/GTSRB_Final_Test_GT.zip\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 99.6k/99.6k [00:00<00:00, 221kB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/gtsrb/gtsrb/GTSRB_Final_Test_GT.zip to data/gtsrb/gtsrb\n",
            "Epoch: 0 \t Iteration: 0 \t Train Loss: 3.7434420585632324\n",
            "Epoch: 0 \t Iteration: 100 \t Train Loss: 2.390298366546631\n",
            "Epoch: 0 \t Iteration: 200 \t Train Loss: 2.4585907459259033\n",
            "Epoch: 0 \t Iteration: 300 \t Train Loss: 1.5240609645843506\n",
            "Epoch: 0 \t Iteration: 400 \t Train Loss: 1.3771090507507324\n",
            "Epoch: 1 \t Iteration: 500 \t Train Loss: 1.2007291316986084\n",
            "Epoch: 1 \t Iteration: 600 \t Train Loss: 0.9711872935295105\n",
            "Epoch: 1 \t Iteration: 700 \t Train Loss: 0.8612615466117859\n",
            "Epoch: 1 \t Iteration: 800 \t Train Loss: 0.8232224583625793\n",
            "Epoch: 2 \t Iteration: 900 \t Train Loss: 0.8351987600326538\n",
            "Epoch: 2 \t Iteration: 1000 \t Train Loss: 0.5061237215995789\n",
            "Epoch: 2 \t Iteration: 1100 \t Train Loss: 0.6144967675209045\n",
            "Epoch: 2 \t Iteration: 1200 \t Train Loss: 0.555315375328064\n",
            "Epoch: 3 \t Iteration: 1300 \t Train Loss: 0.5759874582290649\n",
            "Epoch: 3 \t Iteration: 1400 \t Train Loss: 0.6851558089256287\n",
            "Epoch: 3 \t Iteration: 1500 \t Train Loss: 0.4811170995235443\n",
            "Epoch: 3 \t Iteration: 1600 \t Train Loss: 1.1133809089660645\n",
            "Epoch: 4 \t Iteration: 1700 \t Train Loss: 0.15546433627605438\n",
            "Epoch: 4 \t Iteration: 1800 \t Train Loss: 0.7185407280921936\n",
            "Epoch: 4 \t Iteration: 1900 \t Train Loss: 0.24712955951690674\n",
            "Epoch: 4 \t Iteration: 2000 \t Train Loss: 0.3294205367565155\n",
            "Epoch: 5 \t Iteration: 2100 \t Train Loss: 0.4915965497493744\n",
            "Epoch: 5 \t Iteration: 2200 \t Train Loss: 0.44925999641418457\n",
            "Epoch: 5 \t Iteration: 2300 \t Train Loss: 0.5982107520103455\n",
            "Epoch: 5 \t Iteration: 2400 \t Train Loss: 0.6732851266860962\n",
            "Epoch: 6 \t Iteration: 2500 \t Train Loss: 0.24863247573375702\n",
            "Epoch: 6 \t Iteration: 2600 \t Train Loss: 0.5816096067428589\n",
            "Epoch: 6 \t Iteration: 2700 \t Train Loss: 0.37841805815696716\n",
            "Epoch: 6 \t Iteration: 2800 \t Train Loss: 0.3372187316417694\n",
            "Epoch: 6 \t Iteration: 2900 \t Train Loss: 0.6197717189788818\n",
            "Epoch: 7 \t Iteration: 3000 \t Train Loss: 0.6396780014038086\n",
            "Epoch: 7 \t Iteration: 3100 \t Train Loss: 0.3901028335094452\n",
            "Epoch: 7 \t Iteration: 3200 \t Train Loss: 0.5116944313049316\n",
            "Epoch: 7 \t Iteration: 3300 \t Train Loss: 0.5223680138587952\n",
            "Epoch: 8 \t Iteration: 3400 \t Train Loss: 0.330544114112854\n",
            "Epoch: 8 \t Iteration: 3500 \t Train Loss: 0.20005491375923157\n",
            "Epoch: 8 \t Iteration: 3600 \t Train Loss: 0.41173985600471497\n",
            "Epoch: 8 \t Iteration: 3700 \t Train Loss: 0.2599204182624817\n",
            "Epoch: 9 \t Iteration: 3800 \t Train Loss: 0.48311731219291687\n",
            "Epoch: 9 \t Iteration: 3900 \t Train Loss: 0.2736145555973053\n",
            "Epoch: 9 \t Iteration: 4000 \t Train Loss: 0.5071871876716614\n",
            "Epoch: 9 \t Iteration: 4100 \t Train Loss: 0.3306185007095337\n",
            "Test Accuracy: 74.40%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lu7a6tteuoI7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import snntorch as snn\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "import snntorch.functional as SF\n",
        "from torchvision.datasets import ImageFolder\n",
        "\n",
        "# Define Constants\n",
        "beta = 0.9\n",
        "num_steps = 50  # Number of time steps\n",
        "num_classes = 43\n",
        "batch_size = 64\n",
        "dtype = torch.float\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Define Transformations for GTSRB with Data Augmentation\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((32, 32)),                       # Resize images to 32x32 pixels\n",
        "    transforms.ColorJitter(brightness=0.2,             # Augment brightness, contrast, etc.\n",
        "                           contrast=0.2,\n",
        "                           saturation=0.2,\n",
        "                           hue=0.1),\n",
        "    transforms.RandomRotation(10),                     # Random rotation up to 10 degrees\n",
        "    transforms.ToTensor(),                             # Convert images to PyTorch tensors\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalize\n",
        "])\n",
        "\n",
        "# Load GTSRB dataset from torchvision\n",
        "data_path = './data/gtsrb'  # Set your data path\n",
        "train_data = datasets.GTSRB(root=data_path, split='train', download=False, transform=transform)\n",
        "test_data = datasets.GTSRB(root=data_path, split='test', download=False, transform=transform)\n",
        "train_loader = DataLoader(train_data, batch_size=64, shuffle=True, drop_last=True)  # Smaller batch size\n",
        "test_loader = DataLoader(test_data, batch_size=64, shuffle=True, drop_last=True)\n",
        "\n",
        "# Define Spiking CNN Network\n",
        "class ConvNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        # Convolutional and Pooling Layers with Leaky Integrate-and-Fire (LIF) Neurons\n",
        "        self.conv1 = nn.Conv2d(3, 8, kernel_size=5, padding=\"same\")  # 3 input channels for RGB\n",
        "        self.lif1 = snn.Leaky(beta=beta)\n",
        "        self.mp1 = nn.MaxPool2d(2)\n",
        "\n",
        "        self.conv2 = nn.Conv2d(8, 24, kernel_size=5, padding=\"same\")\n",
        "        self.lif2 = snn.Leaky(beta=beta)\n",
        "        self.mp2 = nn.MaxPool2d(2)\n",
        "\n",
        "        # Fully Connected Layer\n",
        "        self.fc = nn.Linear(24 * 8 * 8, num_classes)  # Adjusted for 32x32 input, after pooling\n",
        "        self.lif3 = snn.Leaky(beta=beta)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Initialize hidden states at t=0 for each LIF layer\n",
        "        mem1 = self.lif1.init_leaky()\n",
        "        mem2 = self.lif2.init_leaky()\n",
        "        mem3 = self.lif3.init_leaky()\n",
        "\n",
        "        # Record the final layer\n",
        "        spk3_rec = []\n",
        "        mem3_rec = []\n",
        "\n",
        "        # Time-step loop\n",
        "        for step in range(num_steps):\n",
        "            cur1 = self.conv1(x)\n",
        "            spk1, mem1 = self.lif1(self.mp1(cur1), mem1)\n",
        "            cur2 = self.conv2(spk1)\n",
        "            spk2, mem2 = self.lif2(self.mp2(cur2), mem2)\n",
        "            cur3 = self.fc(spk2.flatten(1))\n",
        "            spk3, mem3 = self.lif3(cur3, mem3)\n",
        "\n",
        "            spk3_rec.append(spk3)\n",
        "            mem3_rec.append(mem3)\n",
        "\n",
        "        return torch.stack(spk3_rec, dim=0), torch.stack(mem3_rec, dim=0)\n",
        "\n",
        "# Initialize Network\n",
        "convnet = ConvNet().to(device)\n",
        "\n",
        "# Loss and Optimizer\n",
        "loss = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(convnet.parameters(), lr=1e-3)\n",
        "\n",
        "# Training Loop\n",
        "num_epochs = 10\n",
        "loss_hist = []\n",
        "counter = 0\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    for data, targets in train_loader:\n",
        "        data = data.to(device)\n",
        "        targets = targets.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        convnet.train()\n",
        "        spk_rec, _ = convnet(data)\n",
        "\n",
        "        # Calculate loss and perform optimization\n",
        "        loss_val = loss(spk_rec.sum(0), targets)  # Sum spikes over time\n",
        "        optimizer.zero_grad()\n",
        "        loss_val.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        loss_hist.append(loss_val.item())\n",
        "\n",
        "        # Print progress\n",
        "        if counter % 100 == 0:\n",
        "            print(f\"Iteration: {counter}  Train Loss: {loss_val.item()}\")\n",
        "        counter += 1\n",
        "\n",
        "# Accuracy Measurement Function\n",
        "def measure_accuracy(model, dataloader):\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        for data, targets in dataloader:\n",
        "            data = data.to(device)\n",
        "            targets = targets.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            spk_rec, _ = model(data)\n",
        "            spike_count = spk_rec.sum(0)\n",
        "            _, predicted = spike_count.max(1)\n",
        "\n",
        "            correct += (predicted == targets).sum().item()\n",
        "            total += targets.size(0)\n",
        "\n",
        "    return correct / total\n",
        "\n",
        "# Evaluate Network\n",
        "accuracy = measure_accuracy(convnet, test_loader)\n",
        "print(f\"ConvNet Accuracy on GTSRB with Augmentation: {accuracy * 100:.2f}%\")\n"
      ],
      "metadata": {
        "id": "um8YT8vxu4jR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ef399cf-be33-410c-84fb-cb24a9022a02"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration: 0  Train Loss: 7.057320594787598\n",
            "Iteration: 100  Train Loss: 2.316955089569092\n",
            "Iteration: 200  Train Loss: 1.8013885021209717\n",
            "Iteration: 300  Train Loss: 0.5345782041549683\n",
            "Iteration: 400  Train Loss: 0.5596723556518555\n",
            "Iteration: 500  Train Loss: 0.8023688793182373\n",
            "Iteration: 600  Train Loss: 0.38728249073028564\n",
            "Iteration: 700  Train Loss: 0.18824245035648346\n",
            "Iteration: 900  Train Loss: 0.3308257460594177\n",
            "Iteration: 1000  Train Loss: 0.4738938808441162\n",
            "Iteration: 1100  Train Loss: 0.22250866889953613\n",
            "Iteration: 1200  Train Loss: 0.07503364980220795\n",
            "Iteration: 1300  Train Loss: 0.16306276619434357\n",
            "Iteration: 1400  Train Loss: 0.2640307545661926\n",
            "Iteration: 1500  Train Loss: 0.08506952971220016\n",
            "Iteration: 1600  Train Loss: 0.08784216642379761\n",
            "Iteration: 1700  Train Loss: 0.1064421683549881\n",
            "Iteration: 1800  Train Loss: 0.077568918466568\n",
            "Iteration: 1900  Train Loss: 0.07912462204694748\n",
            "Iteration: 2000  Train Loss: 0.09290442615747452\n",
            "Iteration: 2100  Train Loss: 0.135872945189476\n",
            "Iteration: 2200  Train Loss: 0.10431217402219772\n",
            "Iteration: 2300  Train Loss: 0.1497911959886551\n",
            "Iteration: 2400  Train Loss: 0.10731577128171921\n",
            "Iteration: 2500  Train Loss: 0.16649235785007477\n",
            "Iteration: 2600  Train Loss: 0.0263407900929451\n",
            "Iteration: 2700  Train Loss: 0.06544549763202667\n",
            "Iteration: 2800  Train Loss: 0.04847873002290726\n",
            "Iteration: 2900  Train Loss: 0.035807251930236816\n",
            "Iteration: 3000  Train Loss: 0.08982497453689575\n",
            "Iteration: 3100  Train Loss: 0.029164686799049377\n",
            "Iteration: 3200  Train Loss: 0.007909907028079033\n",
            "Iteration: 3300  Train Loss: 0.020200153812766075\n",
            "Iteration: 3400  Train Loss: 0.03209222853183746\n",
            "Iteration: 3500  Train Loss: 0.06612294167280197\n",
            "Iteration: 3600  Train Loss: 0.08741537481546402\n",
            "Iteration: 3700  Train Loss: 0.040260907262563705\n",
            "Iteration: 3800  Train Loss: 0.012478910386562347\n",
            "Iteration: 3900  Train Loss: 0.07254710793495178\n",
            "Iteration: 4000  Train Loss: 0.098276786506176\n",
            "Iteration: 4100  Train Loss: 0.001954633742570877\n",
            "ConvNet Accuracy on GTSRB with Augmentation: 86.12%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uKnIlKs5NWr0"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CkmETmwkNgHI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}